{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MixMatch PyTorch Implementation"]},{"cell_type":"markdown","metadata":{},"source":["## 필요한 패키지 다운로드"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"QFHP0Js1JBSs"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n","Requirement already satisfied: colorama in /usr/local/lib/python3.8/dist-packages (0.4.6)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n","You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["! pip install colorama\n","! git clone https://github.com/hansam95/LG-Elec-Day18.git"]},{"cell_type":"markdown","metadata":{},"source":["## 필요한 패키지 중 이미 다운 받아진 패키지 부르기"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"4gCQgfpOHkb7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os, math, sys, argparse\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","\n","from tqdm import tqdm\n","from colorama import Fore\n","from torch.utils.data import DataLoader\n","from torchvision import transforms as transforms"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def Normalize(x, m=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2345, 0.2616)):\n","    # m: mean, std: standard deviation\n","    # (mean, std) 사전에 구해진 값이기에 따로 구할 필요 없음\n","    # MixMatch를 개인적으로 사용하시는 분은 새롭게 구해야 함\n","    # ex: 변수가 10개인 tablur data에 적용한다면, 변수 별 mean, std를 구해주어야함\n","    # from torchvision.transforms 내 Normalize 함수와 동일한 함수\n","    x, m, std = [np.array(a, np.float32) for a in (x, m, std)]\n","\n","    x -= m * 255\n","    x *= 1.0/(255*std)\n","    return x\n","\n","\n","def Transpose(x, source='NHWC', target='NCHW'):\n","    # N, H, W, C = Batch_size, Height, Width, # of channels\n","    # 일반적인 이미지의 경우 3\n","    # torch.nn.Conv2d는 (N, C, H, W) 의 형태를 가진 데이터를 입력 받기 때문에 형태 변경\n","    # from torchvision.transforms 내 ToTensor 와 동일한 함수\n","    return x.transpose([source.index(d) for d in target])\n","\n","\n","def pad(x, border=4):\n","    # 특정 이미지에 동서남북 방향으로 4만큼 픽셀을 추가해 붙여주는 작업\n","    # 이를 padding이라 부름\n","    return np.pad(x, [(0, 0), (border, border), (border, border)], mode='reflect')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3sOUFnwmHmHc"},"outputs":[],"source":["class Labeled_CIFAR10(torchvision.datasets.CIFAR10):\n","    def __init__(self, root, indices=None,\n","                train=True, transform=None,\n","                target_transform=None, download=False):\n","        super(Labeled_CIFAR10, self).__init__(root,\n","                                        train=train,\n","                                        transform=transform,\n","                                        target_transform=target_transform,\n","                                        download=download)\n","\n","        # label 데이터로 지정된 index\n","        if indices is not None:\n","            self.data = self.data[indices]\n","            self.targets = np.array(self.targets)[indices]\n","        \n","        self.data = Transpose(Normalize(self.data))\n","    \n","    def __getitem__(self, index):\n","        img, target = self.data[index], self.targets[index]\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        \n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","        \n","        return img, target"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"lYm7Pt1vHmUL"},"outputs":[],"source":["class ToTensor(object):\n","    def __call__(self, x):\n","        x = torch.from_numpy(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## WideResNet"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_a9lgAOhHmeF"},"outputs":[],"source":["class BasicBlock(nn.Module):\n","    def __init__(self, in_planes, out_planes, stride, dropRate=0.0, activate_before_residual=False):\n","        super(BasicBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes, momentum=0.001)\n","        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n","        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_planes, momentum=0.001)\n","        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n","        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n","                               padding=1, bias=False)\n","        self.droprate = dropRate\n","        self.equalInOut = (in_planes == out_planes)\n","        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n","                               padding=0, bias=False) or None\n","        self.activate_before_residual = activate_before_residual\n","    def forward(self, x):\n","        if not self.equalInOut and self.activate_before_residual == True:\n","            x = self.relu1(self.bn1(x))\n","        else:\n","            out = self.relu1(self.bn1(x))\n","        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n","        if self.droprate > 0:\n","            out = F.dropout(out, p=self.droprate, training=self.training)\n","        out = self.conv2(out)\n","        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n","    \n","\n","class NetworkBlock(nn.Module):\n","    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0, activate_before_residual=False):\n","        super(NetworkBlock, self).__init__()\n","        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate, activate_before_residual)\n","    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate, activate_before_residual):\n","        layers = []\n","        for i in range(int(nb_layers)):\n","            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate, activate_before_residual))\n","        return nn.Sequential(*layers)\n","    def forward(self, x):\n","        return self.layer(x)\n","    \n","\n","class WideResNet(nn.Module):\n","    def __init__(self, num_classes, depth=28, widen_factor=2, dropRate=0.0):\n","        super(WideResNet, self).__init__()\n","        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor] # 채널수\n","        assert((depth - 4) % 6 == 0)\n","        n = (depth - 4) / 6\n","        block = BasicBlock\n","        # 1st conv before any network block\n","        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n","                               padding=1, bias=False)\n","        # 1st block\n","        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate, activate_before_residual=True)\n","        # 2nd block\n","        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n","        # 3rd block\n","        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n","        # global average pooling and classifier\n","        self.bn1 = nn.BatchNorm2d(nChannels[3], momentum=0.001)\n","        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n","        self.fc = nn.Linear(nChannels[3], num_classes)\n","        self.nChannels = nChannels[3]\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                nn.init.xavier_normal_(m.weight.data)\n","                m.bias.data.zero_()\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.block1(out)\n","        out = self.block2(out)\n","        out = self.block3(out)\n","        out = self.relu(self.bn1(out))\n","        out = F.avg_pool2d(out, 8)\n","        out = out.view(-1, self.nChannels)\n","        return self.fc(out)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"0ekZyGFLI7MS"},"outputs":[],"source":["def get_tqdm_config(total, leave=True, color='white'):\n","    fore_colors = {\n","        'red': Fore.LIGHTRED_EX,\n","        'green': Fore.LIGHTGREEN_EX,\n","        'yellow': Fore.LIGHTYELLOW_EX,\n","        'blue': Fore.LIGHTBLUE_EX,\n","        'magenta': Fore.LIGHTMAGENTA_EX,\n","        'cyan': Fore.LIGHTCYAN_EX,\n","        'white': Fore.LIGHTWHITE_EX,\n","    }\n","    return {\n","        'file': sys.stdout,\n","        'total': total,\n","        'desc': \" \",\n","        'dynamic_ncols': True,\n","        'bar_format':\n","            \"{l_bar}%s{bar}%s| [{elapsed}<{remaining}, {rate_fmt}{postfix}]\" % (fore_colors[color], Fore.RESET),\n","        'leave': leave\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation metric\n","#### top1 accuracy, top5 accuracy\n","#### top1 accuracy: (확률 값이 가장 높은 범주와 실제 범주가 일치하는 관측치 수)/ 전체 관측치\n","#### top5 accuracy: (확률 값 상위 5개 중 실제 범주가 존재하는 관측치 수)/ 전체 관측치"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"6sDCabk8JaGy"},"outputs":[],"source":["def accuracy(output, target, topk=(1, )):\n","    maxk = max(topk)\n","    batch_size = target.size(0)\n","\n","    _, pred = output.topk(maxk, 1, True, True)\n","    pred = pred.t()\n","    correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","    res = []\n","    for k in topk:\n","        if k == 1:\n","            correct_k = correct[:k].view(-1).float().sum(0)\n","        if k > 1:\n","            correct_k = correct[:k].float().sum(0).sum(0)\n","        acc = correct_k.mul_(100.0 / batch_size)\n","        acc = acc.detach().cpu().numpy()\n","        res.append(acc)\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MixMatchTester():\n","    def __init__(self, args):\n","        self.args = args\n","\n","        # root_dir = './MixMatch'\n","        root_dir = '/content/LG-Elec-Day18/MixMatch'\n","\n","        # Data\n","        print(\"==> Preparing CIFAR10 dataset\")\n","        transform_val = transforms.Compose([\n","            ToTensor()\n","        ]) # validation, test dataset에 대한 data augmentation 정의\n","           # 합성곱 신경망에 입력 될 수 있도록만 지정(Augmentation 사용하지 않는 것과 동일)\n","\n","        test_set = Labeled_CIFAR10(os.path.join(root_dir, 'data'), train=False, transform=transform_val, download=True) # test dataset\n","        \n","        # DataLoader 정의\n","        self.test_loader = DataLoader(\n","            dataset=test_set, shuffle=False, num_workers=0, drop_last=False\n","        )\n","\n","        # Build WideResNet\n","        print(\"==> Preparing WideResNet\")\n","        self.model = self.create_model(ema=False)\n","        self.ema_model = self.create_model(ema=True)\n","\n","        ckpt_path = os.path.join(root_dir, 'pretrain', 'ema_model.pth')\n","        ckpt = torch.load(ckpt_path)\n","        self.ema_model.load_state_dict(ckpt)\n","\n","    def create_model(self, ema=False):\n","        # Build WideResNet & EMA model\n","        model = WideResNet(num_classes=10)\n","        model = model.to(self.args.cuda)\n","\n","        if ema:\n","            for param in model.parameters():\n","                param.detach_()\n","            \n","        return model\n","\n","    @torch.no_grad()\n","    def validate(self):\n","        self.ema_model.eval()     \n","        data_loader = self.test_loader\n","        c = 'red'\n","\n","        top1s, top5s = [], []\n","\n","        with tqdm(**get_tqdm_config(total=len(data_loader),\n","                leave=True, color=c)) as pbar:\n","            for batch_idx, (inputs, targets) in enumerate(data_loader):\n","                inputs, targets = inputs.to(self.args.cuda), targets.to(self.args.cuda)\n","\n","                outputs = self.ema_model(inputs)\n","                # labeled dataset에 대해서만 손실함수 계산\n","                # torch.nn.CrossEntropyLoss()를 사용해서 손실함수 계산\n","\n","                prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n","                top1s.append(prec1)\n","                top5s.append(prec5)\n","\n","                pbar.set_description(\n","                    '[Top1 Acc: %.3f|Top5 Acc: %.3f]'%(np.mean(top1s), np.mean(top5s))\n","                )\n","                pbar.update(1)\n","\n","            pbar.set_description(\n","                '[Top1 Acc: %.3f|Top5 Acc: %.3f]'%(np.mean(top1s), np.mean(top5s))\n","            )\n","\n","        return np.mean(top1s), np.mean(top5s)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class FixMatchTester():\n","    def __init__(self, args):\n","        self.args = args\n","\n","        root_dir = './FixMatch'\n","        root_dir = '/content/LG-Elec-Day18/FixMatch'\n","\n","        # Data\n","        print(\"==> Preparing CIFAR10 dataset\")\n","        transform_val = transforms.Compose([\n","            ToTensor()\n","        ]) # validation, test dataset에 대한 data augmentation 정의\n","           # 합성곱 신경망에 입력 될 수 있도록만 지정(Augmentation 사용하지 않는 것과 동일)\n","\n","        test_set = Labeled_CIFAR10(os.path.join(root_dir, 'data'), train=False, transform=transform_val, download=True) # test dataset\n","        \n","        # DataLoader 정의\n","        self.test_loader = DataLoader(\n","            dataset=test_set, shuffle=False, num_workers=0, drop_last=False\n","        )\n","\n","        # Build WideResNet\n","        print(\"==> Preparing WideResNet\")\n","        self.ema_model = WideResNet(self.args.n_classes).to(self.args.cuda)\n","\n","        ckpt_path = os.path.join(root_dir, 'pretrain', 'ema_model.pth')\n","        ckpt = torch.load(ckpt_path)\n","        self.ema_model.load_state_dict(ckpt)\n","    \n","    # MixMatch validate 함수와 동일\n","    @torch.no_grad()\n","    def validate(self):\n","        self.ema_model.eval()     \n","        data_loader = self.test_loader\n","        c = 'red'\n","\n","        top1s, top5s = [], []\n","\n","        with tqdm(**get_tqdm_config(total=len(data_loader),\n","                leave=True, color=c)) as pbar:\n","            for batch_idx, (inputs, targets) in enumerate(data_loader):\n","                inputs, targets = inputs.to(self.args.cuda), targets.to(self.args.cuda)\n","\n","                outputs = self.ema_model(inputs)\n","                # labeled dataset에 대해서만 손실함수 계산\n","                # torch.nn.CrossEntropyLoss()를 사용해서 손실함수 계산\n","\n","                prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n","                top1s.append(prec1)\n","                top5s.append(prec5)\n","\n","                pbar.set_description(\n","                    '[Top1 Acc: %.3f|Top5 Acc: %.3f]'%(np.mean(top1s), np.mean(top5s))\n","                )\n","                pbar.update(1)\n","\n","            pbar.set_description(\n","                '[Top1 Acc: %.3f|Top5 Acc: %.3f]'%(np.mean(top1s), np.mean(top5s))\n","            )\n","\n","        return np.mean(top1s), np.mean(top5s)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def MixMatch_parser():\n","    parser = argparse.ArgumentParser(description=\"MixMatch PyTorch Implementation for LG Electornics education\")\n","    \n","    # method arguments\n","    parser.add_argument('--n-labeled', type=int, default=1000)\n","    parser.add_argument('--num-iter', type=int, default=1024,\n","                        help=\"The number of iteration per epoch\")\n","    parser.add_argument('--alpha', type=float, default=0.75)\n","    parser.add_argument('--lambda-u', type=float, default=75)\n","    parser.add_argument('--T', default=0.5, type=float)\n","    parser.add_argument('--ema-decay', type=float, default=0.999)\n","\n","    parser.add_argument('--epochs', type=int, default=3)\n","    parser.add_argument('--batch-size', type=int, default=64)\n","    parser.add_argument('--lr', type=float, default=0.002)\n","\n","    return parser"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def FixMatch_parser():\n","    parser = argparse.ArgumentParser(description=\"FixMatch PyTorch Implementation for LG Electornics education\")\n","    \n","    # method arguments\n","    parser.add_argument('--n-labeled', type=int, default=3000)\n","    parser.add_argument('--n-classes', type=int, default=10)\n","    parser.add_argument(\"--expand-labels\", action=\"store_true\",\n","                        help=\"expand labels to fit eval steps\")\n","\n","    # training hyperparameters\n","    parser.add_argument('--batch-size', type=int, default=64)\n","    parser.add_argument('--total-steps', default=2**20, type=int)\n","    parser.add_argument('--eval-step', type=int, default=1024)\n","    parser.add_argument('--lr', type=float, default=0.03)\n","    parser.add_argument('--weight-decay', type=float, default=5e-4)\n","    parser.add_argument('--nesterov', action='store_true', default=True)\n","    parser.add_argument('--warmup', type=float, default=0.0)\n","\n","    parser.add_argument('--use-ema', action='store_true', default=True)\n","    parser.add_argument('--ema-decay', type=float, default=0.999)\n","\n","    parser.add_argument('--mu', type=int, default=7)\n","    parser.add_argument('--T', type=float, default=1.0)\n","\n","    parser.add_argument('--threshold', type=float, default=0.95)\n","    parser.add_argument('--lambda-u', type=float, default=1.0)\n","    return parser"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["==> Preparing CIFAR10 dataset\n","Files already downloaded and verified\n","==> Preparing WideResNet\n","[Top1 Acc: 91.450|Top5 Acc: 99.350]: 100%|\u001b[91m██████████\u001b[39m| [00:22<00:00, 442.34it/s]  \n"]},{"data":{"text/plain":["(91.45, 99.35)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["parser = MixMatch_parser()\n","args = parser.parse_args([])\n","args.cuda = torch.device(\"cuda:0\")\n","tester = MixMatchTester(args)\n","tester.validate()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["==> Preparing CIFAR10 dataset\n","Files already downloaded and verified\n","==> Preparing WideResNet\n","[Top1 Acc: 39.490|Top5 Acc: 87.430]: 100%|\u001b[91m██████████\u001b[39m| [00:22<00:00, 443.52it/s] \n"]},{"data":{"text/plain":["(39.49, 87.43)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["parser = FixMatch_parser()\n","args = parser.parse_args([])\n","args.cuda = torch.device(\"cuda:0\")\n","tester = FixMatchTester(args)\n","tester.validate()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPxnUlhDyQQW8m0x4Udibre","collapsed_sections":[],"name":"Untitled1.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}}},"nbformat":4,"nbformat_minor":2}
